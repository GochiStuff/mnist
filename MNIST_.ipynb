{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOnd+Grd4EuEM/zjB4CG8sZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GochiStuff/mnist/blob/main/MNIST_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "So I will try to build a number identify Netwowrk , and see if everything will go fine !"
      ],
      "metadata": {
        "id": "9Pc62TVTegvQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Starting off I will get the required data !"
      ],
      "metadata": {
        "id": "EipHOKrNerzI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "Jv7jXvh_JZkz"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.datasets import mnist\n",
        "\n",
        "# Load data\n",
        "(X_train_raw, y_train_raw), (X_test_raw, y_test_raw) = mnist.load_data()\n",
        "\n",
        "# Flatten 28x28 images to 784-dim vectors\n",
        "X_train = X_train_raw.reshape(-1, 784).astype(np.float32) / 255.0  # (60000, 784)\n",
        "X_test  = X_test_raw.reshape(-1, 784).astype(np.float32) / 255.0   # (10000, 784)\n"
      ],
      "metadata": {
        "id": "TA0UjapBe0Fk"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let me write what I am going to do ! :\n",
        "\n",
        "1. Z = WX + b\n",
        "  here W -> shape"
      ],
      "metadata": {
        "id": "Xj8ydVjkgba5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# One-hot encode labels\n",
        "def one_hot(y, num_classes=10):\n",
        "    return np.eye(num_classes)[y]\n",
        "\n",
        "Y_train = one_hot(y_train_raw)  # (60000, 10)\n",
        "Y_test  = one_hot(y_test_raw)   # (10000, 10)\n",
        "\n",
        "print(X_train[2])"
      ],
      "metadata": {
        "id": "TX3vpowhjvRT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b79ff3ca-5d28-4fc5-fb6e-a3ec64d7a95f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.2627451  0.9098039\n",
            " 0.15294118 0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.24313726 0.31764707\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.47058824 0.7058824  0.15294118 0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.49411765 0.6392157  0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.00784314\n",
            " 0.6        0.8235294  0.15686275 0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.8627451  0.6392157  0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.10588235 0.99607843 0.63529414\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.87058824 0.6392157\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.7176471  0.99607843 0.49019608 0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.18039216 0.9607843  0.6392157  0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.7764706\n",
            " 0.99607843 0.21960784 0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.47058824\n",
            " 0.99607843 0.6392157  0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.09019608 0.90588236 0.99607843 0.11372549\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.62352943 0.99607843 0.47058824\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.6392157  0.99607843 0.84705883 0.0627451  0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.62352943 0.99607843 0.2627451  0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.05490196 0.3372549  0.69803923 0.972549   0.99607843\n",
            " 0.35686275 0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.62352943\n",
            " 0.99607843 0.33333334 0.         0.         0.         0.18431373\n",
            " 0.19215687 0.45490196 0.5647059  0.5882353  0.94509804 0.9529412\n",
            " 0.91764706 0.7019608  0.94509804 0.9882353  0.15686275 0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.5882353  0.99215686 0.92941177\n",
            " 0.8117647  0.8117647  0.8117647  0.99215686 0.99607843 0.98039216\n",
            " 0.9411765  0.7764706  0.56078434 0.35686275 0.10980392 0.01960784\n",
            " 0.9137255  0.98039216 0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.46666667 0.69411767 0.69411767 0.69411767\n",
            " 0.69411767 0.69411767 0.38431373 0.21960784 0.         0.\n",
            " 0.         0.         0.         0.4        0.99607843 0.8627451\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.6627451  0.99607843 0.5372549  0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.6627451\n",
            " 0.99607843 0.22352941 0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.6627451  0.99607843 0.22352941\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.6627451  1.         0.36862746 0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.6627451\n",
            " 0.99607843 0.3764706  0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.6627451  0.99607843 0.6\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.6627451  1.         0.6        0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.3764706\n",
            " 0.99607843 0.6        0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.        ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# print(X.shape)  # (60000, 784)\n",
        "# print(Y.shape)  # (60000, 10)\n",
        "# print(Y[:20])"
      ],
      "metadata": {
        "id": "9Tt1NIwokC1v"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# starting off with making helper functions"
      ],
      "metadata": {
        "id": "rpNcjqusfyAO"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# -----------------------------\n",
        "# Activation Functions\n",
        "# -----------------------------\n",
        "\n",
        "def sigmoid(z):\n",
        "    \"\"\"Sigmoid activation function.\"\"\"\n",
        "    return 1 / (1 + np.exp(-z))\n",
        "\n",
        "def sigmoid_derivative(z):\n",
        "    \"\"\"Derivative of the sigmoid function.\"\"\"\n",
        "    s = sigmoid(z)\n",
        "    return s * (1 - s)\n",
        "\n",
        "def softmax(z):\n",
        "    \"\"\"Softmax activation function (for output layer).\"\"\"\n",
        "    exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))  # stability\n",
        "    return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
        "\n",
        "def ReLU(x):\n",
        "    \"\"\"ReLU activation function.\"\"\"\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "\n",
        "def ReLU_derivative(x):\n",
        "    return (x > 0).astype(float)\n",
        "# -----------------------------\n",
        "# Loss Function\n",
        "# -----------------------------\n",
        "\n",
        "def cross_entropy_loss(y_true, y_pred):\n",
        "    \"\"\"Cross-entropy loss for multiclass classification.\"\"\"\n",
        "    m = y_true.shape[0]\n",
        "    epsilon = 1e-12  # prevent log(0)\n",
        "    y_pred = np.clip(y_pred, epsilon, 1. - epsilon)\n",
        "    loss = -np.sum(y_true * np.log(y_pred)) / m\n",
        "    return loss"
      ],
      "metadata": {
        "id": "JVCXaUstgBMW"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets start making the forward popagation ."
      ],
      "metadata": {
        "id": "OS3_bgjTI9CF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "important parameters declaration ."
      ],
      "metadata": {
        "id": "rM_aPareJxHL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# W = np.zeros((784, 10))\n",
        "# b = np.zeros(10)"
      ],
      "metadata": {
        "id": "sD2nUCY9J2MA"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def calculate_z( W , X , b) :\n",
        "#   Z = np.dot(X , W) + b\n",
        "#   return Z"
      ],
      "metadata": {
        "id": "6cG-WKZQgPWu"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(calculate_z(W , X , b)[:20])"
      ],
      "metadata": {
        "id": "CWdTPibAgT1W",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "outputId": "b28c6278-81fe-46cf-aec0-738e36de3848"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'calculate_z' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-1a08dcb29e29>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcalculate_z\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'calculate_z' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "time to calculate A"
      ],
      "metadata": {
        "id": "Q69dx70IKzjR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# rather than parts let me just build the complete model now ."
      ],
      "metadata": {
        "id": "H6e7InGAKDc6"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Model:\n",
        "  def __init__(self, X_train, Y_train, epochs=300, learning_rate=0.1, batch_size=128):\n",
        "\n",
        "    self.W1 = np.random.randn(784, 10) * np.sqrt(2. / 784)  # Input to hidden\n",
        "    self.W2 = np.random.randn(10, 10) * np.sqrt(2. / 10)  # Hidden to output\n",
        "    self.b1 = np.zeros(10)\n",
        "    self.b2 = np.zeros(10)\n",
        "    self.batch_size = batch_size\n",
        "    self.epochs = epochs\n",
        "    self.lr = learning_rate\n",
        "    self.X_train = X_train\n",
        "    self.Y_train = Y_train\n",
        "    self.m = X_train.shape[0]\n",
        "\n",
        "\n",
        "  def forward_propagation(self, X):\n",
        "      Z1 = np.dot(X, self.W1) + self.b1\n",
        "      A1 = ReLU(Z1)\n",
        "\n",
        "      Z2 = np.dot(A1, self.W2) + self.b2\n",
        "      A2 = softmax(Z2)\n",
        "\n",
        "      return Z1, A1, Z2, A2\n",
        "\n",
        "\n",
        "  def runModel( self ):\n",
        "\n",
        "    initial_lr = self.lr\n",
        "    for epoch in range(self.epochs):\n",
        "      # shuffle at each epoch\n",
        "      indices = np.random.permutation(self.m)\n",
        "      X_shuffled = self.X_train[indices]\n",
        "      Y_shuffled = self.Y_train[indices]\n",
        "\n",
        "      for i in range ( 0 , self.m , self.batch_size):\n",
        "        X_batch = X_shuffled[i:i+self.batch_size]\n",
        "        Y_batch = Y_shuffled[i:i+self.batch_size]\n",
        "\n",
        "        Z1, A1, Z2, A2 = self.forward_propagation(X_batch)\n",
        "\n",
        "        # Now backprop works:\n",
        "        dZ2 = A2 - Y_batch\n",
        "        dW2 = np.dot(A1.T, dZ2) / self.batch_size\n",
        "        db2 = np.sum(dZ2, axis=0) / self.batch_size\n",
        "\n",
        "        dA1 = np.dot(dZ2, self.W2.T)\n",
        "        dZ1 = dA1 * ReLU_derivative(Z1)\n",
        "        dW1 = np.dot(X_batch.T, dZ1) / self.batch_size\n",
        "        db1 = np.sum(dZ1, axis=0) / self.batch_size\n",
        "\n",
        "\n",
        "        # Update parameters\n",
        "        self.W2 -= self.lr * dW2\n",
        "        self.b2 -= self.lr * db2\n",
        "        self.W1 -= self.lr * dW1\n",
        "        self.b1 -= self.lr * db1\n",
        "\n",
        "\n",
        "\n",
        "      _, _, _, A_all = self.forward_propagation(self.X_train)\n",
        "      loss = cross_entropy_loss(self.Y_train, A_all)\n",
        "      acc = np.mean(np.argmax(A_all, axis=1) == np.argmax(self.Y_train, axis=1))\n",
        "      if epoch % 5 == 0:\n",
        "          print(f\"Epoch {epoch}, Loss: {loss:.4f}, Accuracy: {acc*100:.2f}%\")\n",
        "      self.lr = initial_lr * (0.98 ** (epoch // 10))\n",
        "\n"
      ],
      "metadata": {
        "id": "2hGtSPKRLBrR"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# trying model out !\n",
        "\n",
        "mnist_model = Model(X_train, Y_train)\n",
        "mnist_model.runModel()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0chBt3SHLLP0",
        "outputId": "c72996f8-3532-4c06-9b3f-2001c4ad6ef1"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 0.3583, Accuracy: 89.89%\n",
            "Epoch 5, Loss: 0.2450, Accuracy: 93.02%\n",
            "Epoch 10, Loss: 0.2142, Accuracy: 93.97%\n",
            "Epoch 15, Loss: 0.2039, Accuracy: 94.29%\n",
            "Epoch 20, Loss: 0.1899, Accuracy: 94.58%\n",
            "Epoch 25, Loss: 0.1874, Accuracy: 94.64%\n",
            "Epoch 30, Loss: 0.1811, Accuracy: 94.84%\n",
            "Epoch 35, Loss: 0.1762, Accuracy: 94.89%\n",
            "Epoch 40, Loss: 0.1685, Accuracy: 95.14%\n",
            "Epoch 45, Loss: 0.1718, Accuracy: 94.91%\n",
            "Epoch 50, Loss: 0.1745, Accuracy: 94.86%\n",
            "Epoch 55, Loss: 0.1648, Accuracy: 95.18%\n",
            "Epoch 60, Loss: 0.1681, Accuracy: 94.94%\n",
            "Epoch 65, Loss: 0.1575, Accuracy: 95.36%\n",
            "Epoch 70, Loss: 0.1598, Accuracy: 95.23%\n",
            "Epoch 75, Loss: 0.1552, Accuracy: 95.46%\n",
            "Epoch 80, Loss: 0.1547, Accuracy: 95.47%\n",
            "Epoch 85, Loss: 0.1514, Accuracy: 95.54%\n",
            "Epoch 90, Loss: 0.1504, Accuracy: 95.59%\n",
            "Epoch 95, Loss: 0.1629, Accuracy: 95.07%\n",
            "Epoch 100, Loss: 0.1473, Accuracy: 95.71%\n",
            "Epoch 105, Loss: 0.1486, Accuracy: 95.62%\n",
            "Epoch 110, Loss: 0.1522, Accuracy: 95.52%\n",
            "Epoch 115, Loss: 0.1475, Accuracy: 95.68%\n",
            "Epoch 120, Loss: 0.1487, Accuracy: 95.51%\n",
            "Epoch 125, Loss: 0.1480, Accuracy: 95.63%\n",
            "Epoch 130, Loss: 0.1418, Accuracy: 95.80%\n",
            "Epoch 135, Loss: 0.1447, Accuracy: 95.79%\n",
            "Epoch 140, Loss: 0.1473, Accuracy: 95.54%\n",
            "Epoch 145, Loss: 0.1434, Accuracy: 95.77%\n",
            "Epoch 150, Loss: 0.1439, Accuracy: 95.70%\n",
            "Epoch 155, Loss: 0.1443, Accuracy: 95.71%\n",
            "Epoch 160, Loss: 0.1399, Accuracy: 95.85%\n",
            "Epoch 165, Loss: 0.1455, Accuracy: 95.69%\n",
            "Epoch 170, Loss: 0.1413, Accuracy: 95.78%\n",
            "Epoch 175, Loss: 0.1424, Accuracy: 95.72%\n",
            "Epoch 180, Loss: 0.1410, Accuracy: 95.78%\n",
            "Epoch 185, Loss: 0.1378, Accuracy: 95.93%\n",
            "Epoch 190, Loss: 0.1361, Accuracy: 95.98%\n",
            "Epoch 195, Loss: 0.1364, Accuracy: 95.92%\n",
            "Epoch 200, Loss: 0.1374, Accuracy: 95.89%\n",
            "Epoch 205, Loss: 0.1375, Accuracy: 95.89%\n",
            "Epoch 210, Loss: 0.1367, Accuracy: 95.91%\n",
            "Epoch 215, Loss: 0.1378, Accuracy: 95.89%\n",
            "Epoch 220, Loss: 0.1347, Accuracy: 95.98%\n",
            "Epoch 225, Loss: 0.1387, Accuracy: 95.82%\n",
            "Epoch 230, Loss: 0.1357, Accuracy: 95.91%\n",
            "Epoch 235, Loss: 0.1366, Accuracy: 95.98%\n",
            "Epoch 240, Loss: 0.1355, Accuracy: 95.93%\n",
            "Epoch 245, Loss: 0.1331, Accuracy: 96.08%\n",
            "Epoch 250, Loss: 0.1363, Accuracy: 95.89%\n",
            "Epoch 255, Loss: 0.1326, Accuracy: 96.05%\n",
            "Epoch 260, Loss: 0.1326, Accuracy: 96.08%\n",
            "Epoch 265, Loss: 0.1334, Accuracy: 96.00%\n",
            "Epoch 270, Loss: 0.1335, Accuracy: 96.07%\n",
            "Epoch 275, Loss: 0.1321, Accuracy: 96.14%\n",
            "Epoch 280, Loss: 0.1319, Accuracy: 96.14%\n",
            "Epoch 285, Loss: 0.1307, Accuracy: 96.11%\n",
            "Epoch 290, Loss: 0.1312, Accuracy: 96.10%\n",
            "Epoch 295, Loss: 0.1304, Accuracy: 96.10%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BI4pxZxxP2kU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNPMKOq+/Y+ksMJ2A1a4Rxa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GochiStuff/mnist/blob/main/MNIST_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "So I will try to build a number identify Netwowrk , and see if everything will go fine !"
      ],
      "metadata": {
        "id": "9Pc62TVTegvQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Starting off I will get the required data !"
      ],
      "metadata": {
        "id": "EipHOKrNerzI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "Jv7jXvh_JZkz"
      },
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.datasets import mnist\n",
        "\n",
        "# Load data\n",
        "(X_train_raw, y_train_raw), (X_test_raw, y_test_raw) = mnist.load_data()\n",
        "\n",
        "# Flatten 28x28 images to 784-dim vectors\n",
        "X_train = X_train_raw.reshape(-1, 784).astype(np.float32) / 255.0  # (60000, 784)\n",
        "X_test  = X_test_raw.reshape(-1, 784).astype(np.float32) / 255.0   # (10000, 784)\n"
      ],
      "metadata": {
        "id": "TA0UjapBe0Fk"
      },
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let me write what I am going to do ! :\n",
        "\n",
        "1. Z = WX + b\n",
        "  here W -> shape"
      ],
      "metadata": {
        "id": "Xj8ydVjkgba5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# One-hot encode labels\n",
        "def one_hot(y, num_classes=10):\n",
        "    return np.eye(num_classes)[y]\n",
        "\n",
        "Y_train = one_hot(y_train_raw)  # (60000, 10)\n",
        "Y_test  = one_hot(y_test_raw)   # (10000, 10)\n",
        "\n",
        "# # Final variables\n",
        "# X = X_train  # shape: (60000, 784)\n",
        "# Y = Y_train  # shape: (60000, 10)\n",
        "\n",
        "print(X_train[2])"
      ],
      "metadata": {
        "id": "TX3vpowhjvRT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62339881-e068-4df5-c2cc-b6ff1e6660e6"
      },
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.2627451  0.9098039\n",
            " 0.15294118 0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.24313726 0.31764707\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.47058824 0.7058824  0.15294118 0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.49411765 0.6392157  0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.00784314\n",
            " 0.6        0.8235294  0.15686275 0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.8627451  0.6392157  0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.10588235 0.99607843 0.63529414\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.87058824 0.6392157\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.7176471  0.99607843 0.49019608 0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.18039216 0.9607843  0.6392157  0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.7764706\n",
            " 0.99607843 0.21960784 0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.47058824\n",
            " 0.99607843 0.6392157  0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.09019608 0.90588236 0.99607843 0.11372549\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.62352943 0.99607843 0.47058824\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.6392157  0.99607843 0.84705883 0.0627451  0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.62352943 0.99607843 0.2627451  0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.05490196 0.3372549  0.69803923 0.972549   0.99607843\n",
            " 0.35686275 0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.62352943\n",
            " 0.99607843 0.33333334 0.         0.         0.         0.18431373\n",
            " 0.19215687 0.45490196 0.5647059  0.5882353  0.94509804 0.9529412\n",
            " 0.91764706 0.7019608  0.94509804 0.9882353  0.15686275 0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.5882353  0.99215686 0.92941177\n",
            " 0.8117647  0.8117647  0.8117647  0.99215686 0.99607843 0.98039216\n",
            " 0.9411765  0.7764706  0.56078434 0.35686275 0.10980392 0.01960784\n",
            " 0.9137255  0.98039216 0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.46666667 0.69411767 0.69411767 0.69411767\n",
            " 0.69411767 0.69411767 0.38431373 0.21960784 0.         0.\n",
            " 0.         0.         0.         0.4        0.99607843 0.8627451\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.6627451  0.99607843 0.5372549  0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.6627451\n",
            " 0.99607843 0.22352941 0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.6627451  0.99607843 0.22352941\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.6627451  1.         0.36862746 0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.6627451\n",
            " 0.99607843 0.3764706  0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.6627451  0.99607843 0.6\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.6627451  1.         0.6        0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.3764706\n",
            " 0.99607843 0.6        0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.        ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# print(X.shape)  # (60000, 784)\n",
        "# print(Y.shape)  # (60000, 10)\n",
        "# print(Y[:20])"
      ],
      "metadata": {
        "id": "9Tt1NIwokC1v"
      },
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# starting off with making helper functions"
      ],
      "metadata": {
        "id": "rpNcjqusfyAO"
      },
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# -----------------------------\n",
        "# Activation Functions\n",
        "# -----------------------------\n",
        "\n",
        "def sigmoid(z):\n",
        "    \"\"\"Sigmoid activation function.\"\"\"\n",
        "    return 1 / (1 + np.exp(-z))\n",
        "\n",
        "def sigmoid_derivative(z):\n",
        "    \"\"\"Derivative of the sigmoid function.\"\"\"\n",
        "    s = sigmoid(z)\n",
        "    return s * (1 - s)\n",
        "\n",
        "def softmax(z):\n",
        "    \"\"\"Softmax activation function (for output layer).\"\"\"\n",
        "    exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))  # stability\n",
        "    return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
        "\n",
        "def ReLU(x):\n",
        "    \"\"\"ReLU activation function.\"\"\"\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "\n",
        "def ReLU_derivative(x):\n",
        "    return (x > 0).astype(float)\n",
        "# -----------------------------\n",
        "# Loss Function\n",
        "# -----------------------------\n",
        "\n",
        "def cross_entropy_loss(y_true, y_pred):\n",
        "    \"\"\"Cross-entropy loss for multiclass classification.\"\"\"\n",
        "    m = y_true.shape[0]\n",
        "    epsilon = 1e-12  # prevent log(0)\n",
        "    y_pred = np.clip(y_pred, epsilon, 1. - epsilon)\n",
        "    loss = -np.sum(y_true * np.log(y_pred)) / m\n",
        "    return loss"
      ],
      "metadata": {
        "id": "JVCXaUstgBMW"
      },
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets start making the forward popagation ."
      ],
      "metadata": {
        "id": "OS3_bgjTI9CF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "important parameters declaration ."
      ],
      "metadata": {
        "id": "rM_aPareJxHL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# W = np.zeros((784, 10))\n",
        "# b = np.zeros(10)"
      ],
      "metadata": {
        "id": "sD2nUCY9J2MA"
      },
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def calculate_z( W , X , b) :\n",
        "#   Z = np.dot(X , W) + b\n",
        "#   return Z"
      ],
      "metadata": {
        "id": "6cG-WKZQgPWu"
      },
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(calculate_z(W , X , b)[:20])"
      ],
      "metadata": {
        "id": "CWdTPibAgT1W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5fd7554-4c2b-495c-e50a-180d3fc1c758"
      },
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "time to calculate A"
      ],
      "metadata": {
        "id": "Q69dx70IKzjR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# rather than parts let me just build the complete model now ."
      ],
      "metadata": {
        "id": "H6e7InGAKDc6"
      },
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Model:\n",
        "  def __init__(self, X_train, Y_train, epochs=100, learning_rate=0.1, batch_size=128):\n",
        "\n",
        "    self.W1 = np.random.randn(784, 10) * np.sqrt(2. / 784)  # Input to hidden\n",
        "    self.W2 = np.random.randn(10, 10) * np.sqrt(2. / 10)  # Hidden to output\n",
        "    self.b1 = np.zeros(10)\n",
        "    self.b2 = np.zeros(10)\n",
        "    self.batch_size = batch_size\n",
        "    self.epochs = epochs\n",
        "    self.lr = learning_rate\n",
        "    self.X_train = X_train\n",
        "    self.Y_train = Y_train\n",
        "    self.m = X_train.shape[0]\n",
        "\n",
        "\n",
        "  def forward_propagation(self, X):\n",
        "      Z1 = np.dot(X, self.W1) + self.b1\n",
        "      A1 = ReLU(Z1)\n",
        "\n",
        "      Z2 = np.dot(A1, self.W2) + self.b2\n",
        "      A2 = softmax(Z2)\n",
        "\n",
        "      return Z1, A1, Z2, A2\n",
        "\n",
        "\n",
        "  def runModel( self ):\n",
        "\n",
        "    for epoch in range(self.epochs):\n",
        "      # shuffle at each epoch\n",
        "      indices = np.random.permutation(self.m)\n",
        "      X_shuffled = self.X_train[indices]\n",
        "      Y_shuffled = self.Y_train[indices]\n",
        "\n",
        "      for i in range ( 0 , self.m , self.batch_size):\n",
        "        X_batch = X_shuffled[i:i+self.batch_size]\n",
        "        Y_batch = Y_shuffled[i:i+self.batch_size]\n",
        "\n",
        "        Z1, A1, Z2, A2 = self.forward_propagation(X_batch)\n",
        "\n",
        "        # Now backprop works:\n",
        "        dZ2 = A2 - Y_batch\n",
        "        dW2 = np.dot(A1.T, dZ2) / self.batch_size\n",
        "        db2 = np.sum(dZ2, axis=0) / self.batch_size\n",
        "\n",
        "        dA1 = np.dot(dZ2, self.W2.T)\n",
        "        dZ1 = dA1 * ReLU_derivative(Z1)\n",
        "        dW1 = np.dot(X_batch.T, dZ1) / self.batch_size\n",
        "        db1 = np.sum(dZ1, axis=0) / self.batch_size\n",
        "\n",
        "\n",
        "        # Update parameters\n",
        "        self.W2 -= self.lr * dW2\n",
        "        self.b2 -= self.lr * db2\n",
        "        self.W1 -= self.lr * dW1\n",
        "        self.b1 -= self.lr * db1\n",
        "\n",
        "\n",
        "\n",
        "      _, _, _, A_all = self.forward_propagation(self.X_train)\n",
        "      loss = cross_entropy_loss(self.Y_train, A_all)\n",
        "      acc = np.mean(np.argmax(A_all, axis=1) == np.argmax(self.Y_train, axis=1))\n",
        "      if epoch % 5 == 0:\n",
        "          print(f\"Epoch {epoch}, Loss: {loss:.4f}, Accuracy: {acc*100:.2f}%\")\n"
      ],
      "metadata": {
        "id": "2hGtSPKRLBrR"
      },
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# trying model out !\n",
        "\n",
        "mnist_model = Model(X_train, Y_train)\n",
        "mnist_model.runModel()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0chBt3SHLLP0",
        "outputId": "b7216510-942e-4a13-f38f-06abd50b486a"
      },
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 0.3397, Accuracy: 90.32%\n",
            "Epoch 5, Loss: 0.2543, Accuracy: 92.86%\n",
            "Epoch 10, Loss: 0.2260, Accuracy: 93.65%\n",
            "Epoch 15, Loss: 0.2090, Accuracy: 94.14%\n",
            "Epoch 20, Loss: 0.2044, Accuracy: 94.27%\n",
            "Epoch 25, Loss: 0.1984, Accuracy: 94.37%\n",
            "Epoch 30, Loss: 0.1923, Accuracy: 94.49%\n",
            "Epoch 35, Loss: 0.1867, Accuracy: 94.66%\n",
            "Epoch 40, Loss: 0.1820, Accuracy: 94.72%\n",
            "Epoch 45, Loss: 0.1795, Accuracy: 94.74%\n",
            "Epoch 50, Loss: 0.1785, Accuracy: 94.85%\n",
            "Epoch 55, Loss: 0.1775, Accuracy: 94.88%\n",
            "Epoch 60, Loss: 0.1736, Accuracy: 94.92%\n",
            "Epoch 65, Loss: 0.1651, Accuracy: 95.32%\n",
            "Epoch 70, Loss: 0.1650, Accuracy: 95.26%\n",
            "Epoch 75, Loss: 0.1619, Accuracy: 95.36%\n",
            "Epoch 80, Loss: 0.1596, Accuracy: 95.34%\n",
            "Epoch 85, Loss: 0.1638, Accuracy: 95.26%\n",
            "Epoch 90, Loss: 0.1615, Accuracy: 95.38%\n",
            "Epoch 95, Loss: 0.1537, Accuracy: 95.52%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BI4pxZxxP2kU"
      },
      "execution_count": 116,
      "outputs": []
    }
  ]
}